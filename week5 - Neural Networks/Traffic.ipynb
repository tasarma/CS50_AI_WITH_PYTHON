{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Traffic.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Important**\n",
        "This project is using google colab to get better performance.\n",
        "\n",
        "All information taken from here <a href='https://cs50.harvard.edu/ai/2020/projects/5/traffic/'> CS50 AI </a>. \n"
      ],
      "metadata": {
        "id": "J6niL14us2ex"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Background**\n",
        "\n",
        "As research continues in the development of self-driving cars, one of the key challenges is computer vision, allowing these cars to develop an understanding of their environment from digital images. In particular, this involves the ability to recognize and distinguish road signs – stop signs, speed limit signs, yield signs, and more.\n",
        "\n",
        "In this project, you’ll use TensorFlow to build a neural network to classify road signs based on an image of those signs. To do so, you’ll need a labeled dataset: a collection of images that have already been categorized by the road sign represented in them.\n",
        "\n",
        "For more information about the project <a href='https://cs50.harvard.edu/ai/2020/projects/5/traffic/'> Traffic </a>. "
      ],
      "metadata": {
        "id": "ZHsxXNjxpM3K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "run code below to install this project’s dependencies: **opencv-python** for image processing, **scikit-learn** for ML-related functions, and **tensorflow** for neural networks."
      ],
      "metadata": {
        "id": "cNwdzgjtTLi5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Getting Started**"
      ],
      "metadata": {
        "id": "9bDZXZS4q8Fy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Ub_hQmLTSEBQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb8a53c0-9194-4c21-abe2-3ad47fea8d8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wget in /usr/local/lib/python3.7/dist-packages (3.2)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.7.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.7.0)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.7.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.42.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (12.0.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.12.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.17.3)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.7.0)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.22.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.37.0)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.19.5)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.10.0.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.13.3)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow) (1.5.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.8.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.35.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (57.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (3.3.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow) (4.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow) (3.6.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (3.1.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install wget\n",
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import cv2 as cv\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import wget\n",
        "import shutil\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "gk350svsXLDZ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Download the data set for this project and unzip it**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QdLPeBcbgRb0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://cdn.cs50.net/ai/2020/x/projects/5/gtsrb.zip'\n",
        "wget.download(url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "YNxZjO14cxDO",
        "outputId": "326cf580-73fd-4ba3-cf1a-3bd861be9da7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'gtsrb (1).zip'"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shutil.unpack_archive('./gtsrb.zip', './')"
      ],
      "metadata": {
        "id": "UjtL4wx_damI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Understanding**\n",
        "First, take a look at the data set by opening the gtsrb directory. You’ll notice 43 subdirectories in this dataset, numbered 0 through 42. Each numbered subdirectory represents a different category (a different type of road sign). Within each traffic sign’s directory is a collection of images of that type of traffic sign.\n",
        "\n",
        "Next, take a look at traffic.py. In the main function, we accept as command-line arguments a directory containing the data and (optionally) a filename to which to save the trained model. The data and corresponding labels are then loaded from the data directory (via the load_data function) and split into training and testing sets. After that, the get_model function is called to obtain a compiled neural network that is then fitted on the training data. The model is then evaluated on the testing data. Finally, if a model filename was provided, the trained model is saved to disk."
      ],
      "metadata": {
        "id": "YrauE7ozqG5P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Constant variables**"
      ],
      "metadata": {
        "id": "6DiTmCfguGH1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "IMG_WIDTH = 30\n",
        "IMG_HEIGHT = 30\n",
        "NUM_CATEGORIES = 43\n",
        "TEST_SIZE = 0.4"
      ],
      "metadata": {
        "id": "64YJPyuWtzUm"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(data_dir):\n",
        "    \"\"\"\n",
        "    Load image data from directory `data_dir`.\n",
        "\n",
        "    Assume `data_dir` has one directory named after each category, numbered\n",
        "    0 through NUM_CATEGORIES - 1. Inside each category directory will be some\n",
        "    number of image files.\n",
        "\n",
        "    Return tuple `(images, labels)`. `images` should be a list of all\n",
        "    of the images in the data directory, where each image is formatted as a\n",
        "    numpy ndarray with dimensions IMG_WIDTH x IMG_HEIGHT x 3. `labels` should\n",
        "    be a list of integer labels, representing the categories for each of the\n",
        "    corresponding `images`.\n",
        "    \"\"\"\n",
        "    images = []\n",
        "    labels = []\n",
        "\n",
        "    # Loop on all files with directories\n",
        "    for root, subdir, files in os.walk(data_dir):\n",
        "        \n",
        "        # Loop on all images in directory\n",
        "        for image in files:\n",
        "            img = cv.imread(os.path.join(root, image))\n",
        "            img = cv.resize(img, (IMG_WIDTH, IMG_HEIGHT))        \n",
        "            images.append(img)\n",
        "\n",
        "            # Get the directory the image is in\n",
        "            labels.append(int(os.path.basename(root)))\n",
        "    \n",
        "    return images, labels"
      ],
      "metadata": {
        "id": "ziWzfLDbuS9g"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get image arrays and labels for all image files\n",
        "images, labels = load_data('./gtsrb')"
      ],
      "metadata": {
        "id": "qO_55-aPgqot"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of a picture\n",
        "index = 1\n",
        "#print(images.shape)\n",
        "plt.imshow(images[index])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "W29W3zJv0E0G",
        "outputId": "e6a056d1-a375-44f8-b919-fa6fb366806c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f02df08db10>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into training and testing sets\n",
        "labels = tf.keras.utils.to_categorical(labels)\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    np.array(images), np.array(labels), test_size=TEST_SIZE\n",
        ")\n"
      ],
      "metadata": {
        "id": "_poJBiy7wSc9"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m_train = x_train.shape[0] # number of training examples\n",
        "m_test = x_test.shape[0]   # number of test examples\n",
        "num_px = x_train.shape[1]  # = height = width of a training image\n",
        "\n",
        "print (\"Number of training examples: m_train = \" + str(m_train))\n",
        "print (\"Number of testing examples: m_test = \" + str(m_test))\n",
        "print (\"Height/Width of each image: num_px = \" + str(num_px))\n",
        "print (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\n",
        "print (\"x_train shape: \" + str(x_train.shape))\n",
        "print (\"y_train shape: \" + str(y_train.shape))\n",
        "print (\"x_test shape: \" + str(x_test.shape))\n",
        "print (\"y_test shape: \" + str(y_test.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olaUT5uk3QM1",
        "outputId": "cd78b9d0-5141-4f29-cd0e-ff437273b904"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training examples: m_train = 15984\n",
            "Number of testing examples: m_test = 10656\n",
            "Height/Width of each image: num_px = 30\n",
            "Each image is of size: (30, 30, 3)\n",
            "x_train shape: (15984, 30, 30, 3)\n",
            "y_train shape: (15984, 43)\n",
            "x_test shape: (10656, 30, 30, 3)\n",
            "y_test shape: (10656, 43)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model():\n",
        "    \"\"\"\n",
        "    Returns a compiled convolutional neural network model. Assume that the\n",
        "    `input_shape` of the first layer is `(IMG_WIDTH, IMG_HEIGHT, 3)`.\n",
        "    The output layer should have `NUM_CATEGORIES` units, one for each category.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Create a neural network\n",
        "    model = tf.keras.models.Sequential([\n",
        "\n",
        "    # Convolutional layer. Learn 32 filters using a 3x3 kernel\n",
        "    tf.keras.layers.Conv2D(\n",
        "        8, (5, 5), activation=\"relu\", input_shape=(IMG_WIDTH, IMG_HEIGHT, 3)\n",
        "    ),\n",
        "\n",
        "    # Max-pooling layer, using 2x2 pool size\n",
        "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "    # Second convolutional layer\n",
        "    tf.keras.layers.Conv2D(\n",
        "        32, (3, 3), activation=\"relu\", input_shape=(IMG_WIDTH, IMG_HEIGHT, 3)\n",
        "    ),\n",
        "\n",
        "    # Max-pooling layer, using 2x2 pool size\n",
        "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "\n",
        "\n",
        "    # Second convolutional layer\n",
        "    tf.keras.layers.Conv2D(\n",
        "        32, (3, 3), activation=\"relu\", input_shape=(IMG_WIDTH, IMG_HEIGHT, 3)\n",
        "    ),\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Flatten units\n",
        "    tf.keras.layers.Flatten(),\n",
        "\n",
        "    # Add a hidden layer with dropout\n",
        "    tf.keras.layers.Dense(128, activation=\"relu\"),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "\n",
        "    # Add an output layer with output units for all 10 digits\n",
        "    tf.keras.layers.Dense(NUM_CATEGORIES, activation=\"softmax\")\n",
        "\n",
        "    ])\n",
        "\n",
        "    # Summary the model\n",
        "    #model.summary()\n",
        "\n",
        "    # Train neural network\n",
        "    model.compile(\n",
        "        optimizer = \"adam\",\n",
        "        loss = \"categorical_crossentropy\",\n",
        "        metrics = [\"accuracy\"]\n",
        "    )\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "YuKE5uAgujqS"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a compiled neural network\n",
        "model = get_model()\n"
      ],
      "metadata": {
        "id": "hrXRy_U-zdKY"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit model on training data\n",
        "model.fit(x_train, y_train, epochs=EPOCHS)"
      ],
      "metadata": {
        "id": "BtORNdh0zgfD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9343f820-552d-4669-a9c9-a7736f868899"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "500/500 [==============================] - 12s 22ms/step - loss: 3.0814 - accuracy: 0.3070\n",
            "Epoch 2/10\n",
            "500/500 [==============================] - 11s 22ms/step - loss: 1.2726 - accuracy: 0.6465\n",
            "Epoch 3/10\n",
            "500/500 [==============================] - 11s 22ms/step - loss: 0.7756 - accuracy: 0.7812\n",
            "Epoch 4/10\n",
            "500/500 [==============================] - 12s 23ms/step - loss: 0.5176 - accuracy: 0.8525\n",
            "Epoch 5/10\n",
            "500/500 [==============================] - 11s 22ms/step - loss: 0.4179 - accuracy: 0.8813\n",
            "Epoch 6/10\n",
            "500/500 [==============================] - 11s 22ms/step - loss: 0.3382 - accuracy: 0.9012\n",
            "Epoch 7/10\n",
            "500/500 [==============================] - 11s 23ms/step - loss: 0.2898 - accuracy: 0.9150\n",
            "Epoch 8/10\n",
            "500/500 [==============================] - 11s 23ms/step - loss: 0.2426 - accuracy: 0.9284\n",
            "Epoch 9/10\n",
            "500/500 [==============================] - 11s 22ms/step - loss: 0.2387 - accuracy: 0.9338\n",
            "Epoch 10/10\n",
            "500/500 [==============================] - 12s 23ms/step - loss: 0.2069 - accuracy: 0.9398\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f02d3895f10>"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate neural network performance\n",
        "model.evaluate(x_test,  y_test, verbose=2)\n"
      ],
      "metadata": {
        "id": "0Ipw7U8mzih0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a9b0e56-24ac-4aee-ebc5-aaaebcdb006f"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "333/333 - 3s - loss: 0.2007 - accuracy: 0.9479 - 3s/epoch - 8ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.20073269307613373, 0.9479166865348816]"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model to file\n",
        "filename = 'traffic'\n",
        "model.save(filename)"
      ],
      "metadata": {
        "id": "H9WQ7-cfzmxb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}